{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AD_AC207_Documentation_Final.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKxzBafcngft"
      },
      "source": [
        "## **Introduction**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2y_VKYynk_t"
      },
      "source": [
        "This is a package that offers the feature of automatic differentiation.\n",
        "\n",
        "Automatic differentiation is useful in many fields, including but not limit to:\n",
        "\n",
        "- Calculation of derivatives when using some iterative methods to solve linear systems\n",
        "- Calculation of the gradient of an objective function in optimization\n",
        "- Calculation of derivatives/gradients which are parts of some numerical methods to solve differential equation systems\n",
        "\n",
        "Automatic differentiation is better than other differencing methods like finite-difference because it is much cheaper. Finite differences are expensive, since you need to do a forward pass for each derivative. Automatic differentiation is both efficient (linear in the cost of computing the value) and numerically stable. Traditional methods of differentiation such as symbolic differentiation do not scale well to vector functions with multiple variable inputs, which are widely used to solve real world problems.\n",
        "\n",
        "The functions and features in this package can evaluate derivatives/gradients of specified expressions and free users from manual calculation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SuZ93XjunpiR"
      },
      "source": [
        "## **Background**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_O3qdJGnqJ6"
      },
      "source": [
        "For a function, even a complicated one, the computer is able to compute its derivatives by breaking it down into smaller parts, applying chain rule to the elementary operations, and calculate intermerdiate results at each step.\n",
        "\n",
        "In the graph structure of such calculation, each node is an intermediate result, and each arrow is an elementary operation. An elementary operation are such as addition, subtraction, multiplication, division, or taking exponential, log, sine, cosine, etc. In short, AD represent a function as a composition of elementary functions through elemtary operations by a sequence of intermediate values.\n",
        "\n",
        "An example is provided below.\n",
        "\n",
        "\\begin{aligned}\n",
        "&f(x,y) = \\sin(x) - y^2, \\quad v_{-1} = x, \\quad v_0 = y \\\\\n",
        "&v_1 = \\sin(v_{-1}) = \\sin(x), \\quad v_2 = v_0^2 = y^2, v_3 = -v_2 = -y^2, \\quad v_4 = v_1 + v_3 = \\sin(x) - y^2 = f(x,y)\n",
        "\\end{aligned}\n",
        "\n",
        "![AD_example](images/AD_example.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XjRIfUWoogXk"
      },
      "source": [
        "There are two modes of Automatic Differentiation: one is Forward Mode, and the other is Reverse Mode."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LvCNiENBoibo"
      },
      "source": [
        "In forward mode, AD starts from the inputs and work towards the outputs, evaluating the value of each intermediate value along with its derivative with respect to a fixed input variable using the chain rule.\n",
        "\n",
        "$$\\dot{v}_k = \\frac{\\partial{v_k}}{\\partial{x_i}} = \\sum_{v_m \\in \\text{parent}(v_k)} \\frac{\\partial{v_k}}{\\partial{v_m}} \\frac{\\partial{v_m}}{\\partial{x_i}} = \\sum_{v_m \\in \\text{parent}(v_k)} \\frac{\\partial{v_k}}{\\partial{v_m}} \\dot{v}_m$$\n",
        "\n",
        "In the example above, a trace table for forward AD would look like the following to compute and store intermediate values and derivatives:\n",
        "\n",
        "![foward_tracetable_example](https://github.com/cs107-runtimeterror/cs107-FinalProject/blob/final/docs/images/foward_tracetable_example.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OaPgcq8o3Kg"
      },
      "source": [
        "In reverse mode, AD starts from the inputs to do a forward pass to calculate all the intermediate values, and then starts from the outputs to do a reverse pass to compute the derivatives of the function with respect to the intermediate values backwards using the chain rule.\n",
        "\n",
        "$$\\bar{v}_k = \\frac{\\partial{f}}{\\partial{v_k}} = \\sum_{v_n \\in \\text{child}(v_k)} \\frac{\\partial{f}}{\\partial{v_n}} \\frac{\\partial{v_n}}{\\partial{v_k}} = \\sum_{v_n \\in \\text{child}(v_k)} \\bar{v}_n \\frac{\\partial{v_n}}{\\partial{v_k}}$$\n",
        "\n",
        "![reverse_tracetable_example](images/reverse_tracetable_example.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FzTREhhqWzM"
      },
      "source": [
        "## **How to use**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2agmVCfxqaNM"
      },
      "source": [
        "### **Installation**\n",
        "\n",
        "```python\n",
        "[TODO]\n",
        "```\n",
        "\n",
        "You are recommended to use the package under Python version 3.6.2 or later."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYhv05q-qim6"
      },
      "source": [
        "### **Demo**\n",
        "\n",
        "Import package\n",
        "```python\n",
        "import AutoDiff as ad\n",
        "```\n",
        "\n",
        "Specify the names of all the variables that will be used in a list.\n",
        "```python\n",
        "variables = ['x1','x2','x3']\n",
        "```\n",
        "\n",
        "Then, create the variables and specify their values. These variable are just scalar variables. To use them in a vector, just put them in a numpy array.\n",
        "```python\n",
        "x1 = ad.create_node(var='x1', value=2, variables=variables)\n",
        "x2 = ad.create_node(var='x2', value=3, variables=variables)\n",
        "x3 = ad.create_node(var='x3', value=4, variables=variables)\n",
        "```\n",
        "\n",
        "If the target function is a scalar function with a scalar input, calculate the derivative like below:\n",
        "```python\n",
        "y = 3*x1 + x1**2 - exp(x1)\n",
        "der = ad.gradient(y, variables, var='x1')\n",
        "print(f\"The derivative with respect to x1 is {der}\")\n",
        "```\n",
        "```\n",
        "The derivative with respect to x1 is -0.3890560989306504\n",
        "```\n",
        "\n",
        "If the target function is a scalar function with a vector input, calculate the partial derivatives like below:\n",
        "```python\n",
        "y = 3*x1 + x2**2 - exp(x3)\n",
        "der1 = ad.gradient(y, variables, var='x1')\n",
        "der2 = ad.gradient(y, variables, var='x2')\n",
        "der3 = ad.gradient(y, variables, var='x3')\n",
        "grad = ad.gradient(y, variables)\n",
        "print(f\"The partial derivatives with respect to x1, x2, x3 are {der1}, {der2}, and {der3}\")\n",
        "print(f\"The gradient is {grad}\")\n",
        "```\n",
        "```\n",
        "The partial derivatives with respect to x1, x2, x3 are 3.0, 6.0, and -54.598150033144236\n",
        "The gradient is [3.0, 6.0, -54.598150033144236]\n",
        "```\n",
        "\n",
        "\n",
        "If the target function is a vector function with a scalar input, put scalar functions into numpy arrays to form a vector function and calculate the derivatives by specifying `var`:\n",
        "```python\n",
        "y1 = 3*x1 + x1**2 - exp(x1)\n",
        "y2 = log(x1) / sin(x1) + cos(x1)**2\n",
        "y = np.array([y1,y2])\n",
        "grad = ad.gradient(y, variables, var='x1')\n",
        "print(f\"The derivatives are {grad}\")\n",
        "```\n",
        "```\n",
        "The derivatives are [-0.3890560989306504, 1.6555447762793878]\n",
        "```\n",
        "\n",
        "If the target function is a vector function with a vector input, put scalar functions into numpy arrays to form a vector function, and calculate the gradient by not specifying `var`:\n",
        "```python\n",
        "y1 = 3*x1 + x2**2 - exp(x3)\n",
        "y2 = log(x1) / sin(x2) + cos(x3)**2\n",
        "y = np.array([y1,y2])\n",
        "jcb = ad.gradient(y, variables)\n",
        "print(f\"The Jacobian is \\n {jcb}\")\n",
        "```\n",
        "```\n",
        "The Jacobian is \n",
        "[[  3.           6.         -54.59815003]\n",
        " [  3.5430837   34.45721548  -0.98935825]]\n",
        "```\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JimwK5BwBwak"
      },
      "source": [
        "## **Software Organizatoin**\n",
        "\n",
        "### **Directory Structure**\n",
        "# TODO\n",
        "\n",
        "```\n",
        "cs107project/\n",
        "├── LICENSE\n",
        "├── README.md\n",
        "└── AutoDiff/\n",
        "    ├── __init__.py\n",
        "    ├── ad.py\n",
        "    ├── fowardNode.py\n",
        "    ├── reverseNode.py\n",
        "    └── utils.py    \n",
        "└── docs/\n",
        "    ├── milestone1\n",
        "    ├── milestone2_progress\n",
        "    ├── milestone2\n",
        "    └── documentation\n",
        "└── tests/\n",
        "    ├── __init__.py\n",
        "    ├── test_forwardNode.py\n",
        "    └── test_reverseNode.py\n",
        "└── .travis.yml\n",
        "```\n",
        "\n",
        "### **Included Modules and their Basic Functionality**\n",
        "\n",
        "We plan on using NumPy, UnitTest, PyTest and PyTorch. We intend to use NumPy to create matrices and perform elementary calculations, UnitTest and PyTest to run tests on our code, and PyTorch to perform benchmarks on these tests.\n",
        "\n",
        "### **Test Suite**\n",
        "# TODO\n",
        "Our test suite will live a test file /tests directory and it will be tested by CircleCI.\n",
        "\n",
        "### **Package Distribution**\n",
        "We will distribute our package by uploading it to PyPI so everyone can use it.\n",
        "\n",
        "### **Notes**\n",
        "We will not be packing out software. The code will be on GitHub and PyPI so it will be accessible by everyone.\n",
        "\n",
        "As of right now we are still working on this project, so we could potentially make changes to the software later."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gAeoFdAYB6DD"
      },
      "source": [
        "## **Implementation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNZIQ31vB8OU"
      },
      "source": [
        "### **Core Data Structures**\n",
        "\n",
        "Node structure that is able to represent all the intermediate function expressions. Every instance of a Node stores the actual value of the variable, an attibute storing derivatives, as well as the names of the variables that the derivatives are with respect to."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scIQWQKvCWui"
      },
      "source": [
        "### **Classes**\n",
        "\n",
        "1. `class ForwardNode`: This is the most generic base class to accomodate for the different nodes in the AD structure in Forward Mode. \n",
        "\n",
        "2. `class ReverseNode`: This is the most generic base class to accomodate for the different nodes in the AD structure in Reverse Mode. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQNE2tSyi7zk"
      },
      "source": [
        "### **Methods and Name Attributes**\n",
        "\n",
        "The `ForwardNode` class has 3 attributes:\n",
        "- `self.value`: the actual value of the function expression $v_k$ represented by a ForwardNode instance\n",
        "- `self.trace`: a numpy array of the gradients $\\frac{\\partial v_k}{\\partial x_i}$ of this intermediate function expression with respect to the target input variables $x_i$\n",
        "- `self.var`: a list of the names of the variable names $x_i$\n",
        "\n",
        "The `ReverseNode` class has 3 attributes:\n",
        "- `self.value`: the actual value of the function expression $v_k$ represented by a ReverseNode instance\n",
        "- `self.adjoint`: a value of gradient $\\frac{\\partial f_i}{\\partial v_k}$ of the ultimate function expression with respect to the intermediate variable $v_k$\n",
        "- `self.children`: a list of tuples, with each of the tuple storing $v_k$'s children $v_n$ and the derivatives $\\frac{\\partial v_n}{\\partial v_k}$ in the form ($v_n$, $\\frac{\\partial v_n}{\\partial v_k}$)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXfJr-1MjKcd"
      },
      "source": [
        "### **External Dependencies**\n",
        "\n",
        "The implenentation is based heavily on numpy in the overloaded functions to do vectorized operations, and also in the wrap-up functions for easy calculation of gradients."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYbW5FjBCbMu"
      },
      "source": [
        "### **Elementary Functions**\n",
        "\n",
        "The elementary operations are overloaded in this class. Doing any one of the operation would return a new `ForwardNode` instance that represents the new intermediate function expression, and it would contain the attributes mentioned above.\n",
        "\n",
        "Below, `variables` is a list of names of the variables all the functions are derived with respect to. $v_1$ is an instance of ForwardNode, representing a variable (function) having value `value1` and its derivatives with respect to the variables whose names are in `variables` are in `trace1`. $v_2$ is an instance of ForwardNode, representing a variable (function) having value `value2` and its derivatives with respect to the variables whose names are in `variables` are in `trace2`. \n",
        "\n",
        "```python\n",
        "variables = ...\n",
        "v1 = ForwardNode(value1, trace1, variables)\n",
        "v2 = ForwardNode(value2, trace2, variables)\n",
        "```\n",
        "\n",
        "#### **Binary elementary functions**\n",
        "\n",
        "- Addition: $$v_k = v_1 + v_2 \\ \\Rightarrow \\ \\dot{v}_k = 1 \\cdot \\dot{v}_1 + 1 \\cdot \\dot{v}_2$$  \n",
        "```python\n",
        "valuek, tracek = value1+value2, trace1+trace2\n",
        "vk = ForwardNode(valuek, tracek, variables)\n",
        "```\n",
        "- Subtraction: $$v_k = v_1 - v_2 \\ \\Rightarrow \\ \\dot{v}_k = 1 \\cdot \\dot{v}_1 - 1 \\cdot \\dot{v}_2$$  \n",
        "```python\n",
        "valuek, tracek = value1-value2, trace1-trace2\n",
        "vk = ForwardNode(valuek, tracek, variables)\n",
        "```\n",
        "- Multiplication: $$v_k = v_1 \\cdot v_2 \\ \\Rightarrow \\ \\dot{v}_k = v_1 \\cdot \\dot{v}_2 + v_2 \\cdot \\dot{v}_1$$  \n",
        "```python\n",
        "valuek, tracek = value1*value2, value1*trace2+value2*trace1\n",
        "vk = ForwardNode(valuek, tracek, variables)\n",
        "```\n",
        "- Division: $$v_k = \\frac{v_1}{v_2} \\ \\Rightarrow \\ \\dot{v}_k = \\frac{v_2 \\cdot \\dot{v}_1 - v_1 \\cdot \\dot{v}_2}{v_2^2}$$  \n",
        "```python\n",
        "valuek, tracek = value1/value2, (value2*trace1-value1*trace2) / (value2**2)\n",
        "vk = ForwardNode(valuek, tracek, variables)\n",
        "```\n",
        "- Power: $$v_k = v_1^{v_2} \\ \\Rightarrow \\ \\dot{v}_k = v_2 \\cdot v_1^{v_2-1} \\cdot \\dot{v}_1 + v_1^{v_2} \\cdot \\log{v_1} \\cdot \\dot{v}_2$$  \n",
        "```python\n",
        "valuek = value1**value2\n",
        "tracek = value2*(value1**(value2 - 1))*trace1 + (value1**value2)*np.log(value1)*trace2\n",
        "vk = ForwardNode(valuek, tracek, variables)\n",
        "```\n",
        "\n",
        "#### **Unary elementary functions**\n",
        "\n",
        "- Exponential: $$v_k = \\exp{v_1} \\ \\Rightarrow \\ \\dot{v}_k = \\exp{v_1} \\cdot \\dot{v}_1$$ \n",
        "```python\n",
        "valuek, tracek = np.exp(value1), np.exp(value1)*trace1\n",
        "vk = ForwardNode(valuek, tracek, variables)\n",
        "```\n",
        "\n",
        "- Natural log: $$v_k = \\log{v_1} \\ \\Rightarrow \\ \\dot{v}_k = \\frac{\\dot{v}_1}{v_1}$$  \n",
        "```python\n",
        "valuek, tracek = np.log(value1), trace1/value1\n",
        "vk = ForwardNode(valuek, tracek, variables)\n",
        "```\n",
        "\n",
        "- Square root: $$v_k = \\sqrt{v_1} \\ \\Rightarrow \\ \\dot{v}_k = \\frac{1}{2} v_1^{-\\frac{1}{2}} \\cdot \\dot{v}_1$$  \n",
        "```python\n",
        "valuek, tracek = value1**0.5, trace1*0.5*value1**(-0.5)\n",
        "vk = ForwardNode(valuek, tracek, variables)\n",
        "```\n",
        "\n",
        "- Sine: $$v_k = \\sin{v_1} \\ \\Rightarrow \\ \\dot{v}_k = \\cos{v_1} \\cdot \\dot{v}_1$$  \n",
        "```python\n",
        "valuek, tracek = np.sin(value1), np.cos(value1)*trace1\n",
        "vk = ForwardNode(valuek, tracek, variables)\n",
        "```\n",
        "\n",
        "- Cosine: $$v_k = \\cos{v_1} \\ \\Rightarrow \\ \\dot{v}_k = -\\sin{v_1} \\cdot \\dot{v}_1$$   \n",
        "```python\n",
        "valuek, tracek = np.cos(value1), -np.sin(value1)*trace1\n",
        "vk = ForwardNode(valuek, tracek, variables)\n",
        "```\n",
        "\n",
        "- Tangent: $$v_k = \\tan{v_1} \\ \\Rightarrow \\ \\dot{v}_k = \\frac{\\dot{v}_1}{\\cos^2{v_1}}$$   \n",
        "```python\n",
        "valuek, tracek = np.tan(value1), trace1/np.cos(value1)**2\n",
        "vk = ForwardNode(valuek, tracek, variables)\n",
        "```\n",
        "\n",
        "- Cotangent: $$v_k = \\cot{v_1} \\ \\Rightarrow \\ \\dot{v}_k = \\frac{-\\dot{v}_1}{\\sin^2{v_1}}$$   \n",
        "```python\n",
        "valuek, tracek = 1/np.tan(value1), -trace1/np.sin(value1)**2\n",
        "vk = ForwardNode(valuek, tracek, variables)\n",
        "```\n",
        "\n",
        "- Secant: $$v_k = \\sec{v_1} \\ \\Rightarrow \\ \\dot{v}_k = \\frac{\\sin{v_1} \\cdot \\dot{v}_1}{\\cos^2{v_1}}$$   \n",
        "```python\n",
        "valuek, tracek = 1/np.cos(value1), trace1*np.sin(value1)/np.cos(value1)**2\n",
        "vk = ForwardNode(valuek, tracek, variables)\n",
        "```\n",
        "\n",
        "- Cosecant: $$v_k = \\csc{v_1} \\ \\Rightarrow \\ \\dot{v}_k = \\frac{-\\cos{v_1} \\cdot \\dot{v}_1}{\\sin^2{v_1}}$$  \n",
        "```python\n",
        "valuek, tracek = 1/np.sin(value1), -trace1*np.cos(value1)/np.sin(value1)**2\n",
        "vk = ForwardNode(valuek, tracek, variables)\n",
        "```\n",
        "\n",
        "- Arcsine: $$v_k = \\arcsin{v_1} \\ \\Rightarrow \\ \\dot{v}_k = \\frac{\\dot{v}_1}{\\sqrt{1-v_1^2}}$$  \n",
        "```python\n",
        "valuek, tracek = np.arcsin(value1), trace1/np.sqrt(1-value1**2)\n",
        "vk = ForwardNode(valuek, tracek, variables)\n",
        "```\n",
        "\n",
        "- Arccosine: $$v_k = \\arccos{v_1} \\ \\Rightarrow \\ \\dot{v}_k = \\frac{-\\dot{v}_1}{\\sqrt{1-v_1^2}}$$  \n",
        "```python\n",
        "valuek, tracek = np.arccos(value1), -trace1/np.sqrt(1-value1**2)\n",
        "vk = ForwardNode(valuek, tracek, variables)\n",
        "```\n",
        "\n",
        "- Arctangent: $$v_k = \\arctan{v_1} \\ \\Rightarrow \\ \\dot{v}_k = \\frac{\\dot{v}_1}{1+v_1^2}$$  \n",
        "```python\n",
        "valuek, tracek = np.arctan(value1), trace1/(1+value1**2)\n",
        "vk = ForwardNode(valuek, tracek, variables)\n",
        "```\n",
        "\n",
        "- Hyperbolic sine: $$v_k = \\sinh{v_1} \\ \\Rightarrow \\ \\dot{v}_k = \\cosh{v_1} \\cdot \\dot{v}_1$$  \n",
        "```python\n",
        "valuek, tracek = np.sinh(value1), trace1*np.cosh(value1)\n",
        "vk = ForwardNode(valuek, tracek, variables)\n",
        "```\n",
        "\n",
        "- Hyperbolic cosine: $$v_k = \\cosh{v_1} \\ \\Rightarrow \\ \\dot{v}_k = \\sinh{v_1} \\cdot \\dot{v}_1$$  \n",
        "```python\n",
        "valuek, tracek = np.cosh(value1), trace1*np.sinh(value1)\n",
        "vk = ForwardNode(valuek, tracek, variables)\n",
        "```\n",
        "\n",
        "- Hyperbolic tangent: $$v_k = \\tanh{v_1} \\ \\Rightarrow \\ \\dot{v}_k = \\frac{\\dot{v}_1}{\\cosh^2(v_1)}$$  \n",
        "```python\n",
        "valuek, tracek = np.tanh(value1), trace1/np.cosh(value1)**2\n",
        "vk = ForwardNode(valuek, tracek, variables)\n",
        "```\n",
        "\n",
        "- Log of any base $b$: $$v_k = \\log_b v_1 = \\frac{\\log v_1}{\\log b} \\ \\Rightarrow \\ \\dot{v}_k = \\frac{\\dot{v}_1}{v_1 \\cdot \\log b}$$\n",
        "```python\n",
        "valuek, tracek = np.log(value1)/np.log(b), trace1/(value1*np.log(b))\n",
        "vk = ForwardNode(valuek, tracek, variables)\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2Dh5-O2lsly"
      },
      "source": [
        "## **Extension**\n",
        "\n",
        "### **Reflection of Milestone 2 feedback**\n",
        "\n",
        "We added 4 futures features that we planned to implement in our milestone 2 report, and these features are listed below:\n",
        "\n",
        "1. More functions to be overloaded for the ReverseNode class.\n",
        "2. Modify the whole thing so that the derivation function can take in the python function as input but not limited to function expression string input.\n",
        "3. Drawing the graph structure of AD, with nodes (intermediate functions/variables) connected by arrows (elementary operations)\n",
        "4. A more developed version of AD that is able to take in multivariate expression or vector containing multivariate expressions to calculate gradients or Jacobians.\n",
        "\n",
        "#### **Feedback from teaching fellow**\n",
        "\n",
        "Great work. Definitely focus on item 2 and 3 in your immediate future plans. Resolving item 2 would make your package more usable (in the pythonic sense)\n",
        "\n",
        "####  **Reflection**\n",
        "Based on the suggestion from the teaching fellow, we decided to work on adding the python function as an input format for our automatic library to make it more accessible to users. So in our final implementation, the user can either input their functions as a string (a list of strings), or lambda functions. \n",
        "\n",
        "For item 3, we were not able to implement it in our final submission due to the time limit. However, we think it would still be a good idea to develop this feature and allow the users to draw the graph structure of AD as a possible future direction to work on.\n",
        "\n",
        "We also implemented item 1 and 4 in this submission. We overloaded many dunder methods and math functions that we are using for ReverseNode class. We also updated our wrap function call auto_diff() which supports multivariate expression and vector containing multivariate expressions to calculate gradients or Jacobians.\n",
        "\n",
        "### **Description of Extension feature - ReverseNode**\n",
        "\n",
        "Our extension feature allows users to performance automatic differentiation using the reverse mode method. This is achieved through a new class called ReverseNode. The ReverseNode class contains overloaded dunder methods for reverse mode automatic differentiation, such as `__add__`, `__radd__`, `__sub__`, `__pow__`, `__str__`, etc. We also updated the math functions such as `sin`, `cos`, `exp` in our utils.py file to add the calculation for reverse mode. \n",
        "\n",
        "Similar to the design of our `ForwardNode` object, each `ReverseNode` object represents a variable in the function that users input. Each `ReverseNode` will be initialized with the value user select, however, a `ReverseNode` object does not have the `self.trace` and `self.var` attributes as a `ForwardNode` object. Instead, each `ReverseNode` object has attributes called `self.adjoint` and `self.children`. Adjoint keeps trace of the derivative of a ReverseNode object and is always initialized as 1.0. The children attribute is used to record the historical dependency of a node and is initialized as an empty list. \n",
        "\n",
        "The gradient for each `ReverseNode` object is calculated by calling the function `gradient()`. This function calculated the derivative value for a node by iterating through all the nodes in this `self.children` list and sum over the derivative value of all child nodes. The `ReverseNode` class also contains a `gradient_reset()` function, which would reset the `self.adjoint` of a node a 1.0 and `self.children` as an empty list. The `gradient_rese` function is called before calculating the derivative / jacobian of a `ReverseNode` object for a new function when users input a vector function. \n",
        "\n",
        "Finally, `__main__.py` contains the wrap function `auto_diff()` that provides the users a simple method to perform automatic differentiation using either forward and reverse mode. The `auto_diff()` function takes a string (list of string) or lambda functions, dictionary of variable names and values, list indicating target variable, and a string indicating the mode. So to do the reverse mode calculation, the user just need to specify that mode = “reverse”. The `auto_diff()` function will then call `reverse_auto_diff()` function and create corresponding ReverseNode object based on user input, evaluate the input functions, and calculate the derivative for each variables in `gradientR()` function.\n",
        "\n",
        "### **Background of Extention - Reverse Mode Automatic Differentiation**\n",
        "\n",
        "The intuition for implementing a reverse mode method comes from a major disadvantage of the forward mode method. \n",
        "\n",
        "Consider a simple function with 2 variables:\n",
        "\n",
        "$$v_k = v_1 + v_2 \\ \\Rightarrow \\ \\dot{v}_k = 1 \\cdot \\dot{v}_1 + 1 \\cdot \\dot{v}_2$$  \n",
        "\n",
        "$$y = x_1 * x_2 + \\sin(x_1)$$\n",
        "\n",
        "To calculate the derivative $\\frac{\\partial{y}}{\\partial{x_1}}$ and $\\frac{\\partial{y}}{\\partial{x_2}}$, we will need to do 2 forward pass calculation, with the trace being `[1.0, 0.0]` for $\\frac{\\partial{y}}{\\partial{x_1}}$ and `[0.0, 1.0]` for $\\frac{\\partial{y}}{\\partial{x_2}}$. This yields a complexity of `O(N)`, where N equals the number of input variables. So if we need to compute the gradient of a complex function with thousands of variables, it would be computationally expensive to use forward mode method. \n",
        "\n",
        "However, we can actually avoid this problem using the symmetric property of the chain rule:  \n",
        "\n",
        "$$\\frac{\\partial{f}}{\\partial{x_2}} = \\frac{\\partial{f}}{\\partial{x_1}} \\frac{\\partial{x_1}}{\\partial{x_2}}$$\n",
        "\n",
        "To account for this problem, the reverse mode method uses a forward pass to first calculate the partial derivative and store these values in a dependency list, and then proceeds to calculate the derivative of each variable through a reverse pass using the chain rule.\n",
        "\n",
        "\n",
        "$$\\bar{x}_k = \\frac{\\partial{f}}{\\partial{x_k}} = \\sum_{x_i \\in \\text{child}(x_k)} \\frac{\\partial{f}}{\\partial{x_i}} \\frac{\\partial{x_i}}{\\partial{x_k}} = \\sum_{x_i \\in \\text{child}(x_k)} \\bar{x}_i \\frac{\\partial{x_i}}{\\partial{x_k}}$$\n",
        "\n",
        "One useful scenario for reverse mode automatic differentiation in the real-world would be calculating the gradient in deep learning models for video processing or image recognition. In these kind of problem, there are typically thousand or even millions of input representing the image or video pixels and for classification problem there could be only a few output representing the category of classes. So to calculate the gradient of each variables and find our the best direction for gradient descent, a reverse mode automatic differentation is usually used to achieve the best runtime. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GH6vuoyp8Hpy"
      },
      "source": [
        "## **Broader Impact and Inclusivity statement**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opHQ6hSl8QVw"
      },
      "source": [
        "Over the past few years, people have put in an increased effort to bridge the gap in STEM between underrepresented groups and inclusivity. However, even with this increased effort there is much more that can and should be done to fill this gap. While creating our software, we kept in mind that people from different backgrounds and experience levels would access this. Therefore, we tried to add docstrings and the proper documentation in order to make this software as accessible as possible. However, we do understand that there is more work that needs to be done to make our software more accessible and user-friendly. Currently, our software is targeted towards those who are familiar with English mathematical terms and symbols. Our software is catered towards the average English speaker. Moving forward, to make our software more inclusive, we would try to make it more accessible for those who are not as familiar with the English language.\n",
        "\n",
        "We understand that our software has both positive and negative implications. However, we believe the positive implications outweigh the negative ones. Our team has simply found one way to tackle the problem using Automatic Differentiation and believe that we are adding to the diversity of technology in the community by contributing our software. However, we do understand that there is more that can be done to make this software inclusive as mentioned before.\n",
        "\n",
        "Furthermore, Harvard's diversity statement says, \"[their] commitment to diversity in all forms is rooted in [the] fundamental belief that engaging with unfamiliar ideas, perspectives, cultures, and people creates the conditions for dramatic and meaningful growth.\" Our team believes that by engaging with the software we have created, we are sharing our ideas and perspectives on a certain way to solve a problem. However, we are open to suggestions and any feedback our users have. We are constantly seeking to improve the way we implemented our software."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynBCSBnt8UMt"
      },
      "source": [
        "## **Future**\n",
        "\n",
        " For future implementation, we think it would be a good idea to continue working on the feature for drawing the graph structure of automatic differention process with nodes (intermediate functions/variables) connected by arrows (elementary operations) since we are not able to complete for our final submission."
      ]
    }
  ]
}
